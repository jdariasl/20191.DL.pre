{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated BPTT\n",
    "\n",
    "When the sequences are very long (thousands of points), the network training can be very slow and the memory requirements increase. The truncated BPTT is an alternative similar to mini-batch training in Dense Networks, even though in RNN the batch parameter can also be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./Images/rnn_tbptt_2.png \"Truncated BPTT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TBPTT can be implemented by setting up the data appropriately. Let's remember that for recurrent neural networks, data must have the format **[n_samples,n_times,n_features]**, so if you want to use Truncated BPTT you just have to split the sequences into more **n_samples** of less **n_times**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BUT**, **Is it possible that the LSTM may find dependencies between the sequences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it’s not possible unless you go for the stateful LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the use of Truncated BPTT requires to set up the **Stateful** mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Extracted from: http://philipperemy.github.io/keras-stateful-lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also a problem of classifying sequences. The data matrix $X$ is made exclusively of zeros except in the first column where exactly half of the values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_train = 1000\n",
    "X_train = np.zeros((N_train,20))\n",
    "from numpy.random import choice\n",
    "one_indexes = choice(a=N_train, size=int(N_train / 2), replace=False)\n",
    "X_train[one_indexes, 0] = 1  # very long term memory.\n",
    "#--------------------------------\n",
    "N_test = 200\n",
    "X_test = np.zeros((N_test,20))\n",
    "from numpy.random import choice\n",
    "one_indexes = choice(a=N_test, size=int(N_test / 2), replace=False)\n",
    "X_test[one_indexes, 0] = 1  # very long term memory.\n",
    "print(X_train[:10,:2])\n",
    "print(X_test[:10,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(x_train, y_train, window_length, increment):\n",
    "    windows = []\n",
    "    windows_y = []\n",
    "    for i, sequence in enumerate(x_train):\n",
    "        len_seq = len(sequence)\n",
    "        for window_start in range(0, len_seq - window_length + 1, increment):\n",
    "            window_end = window_start + window_length\n",
    "            window = sequence[window_start:window_end]\n",
    "            windows.append(window)\n",
    "            windows_y.append(y_train[i])\n",
    "    return np.array(windows), np.array(windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10, 1)\n",
      "(2000, 1)\n",
      "(400, 10, 1)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split the sequences into two sequences of length 10\n",
    "window_length = 10\n",
    "\n",
    "x_train, y_train = prepare_sequences(X_train, X_train[:,0], window_length,window_length)\n",
    "x_test, y_test = prepare_sequences(X_test, X_test[:,0], window_length,window_length)\n",
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every sequence was split into 2 subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:4,:])\n",
    "print(y_train[:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a regular LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the original sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building STATELESS model...\n",
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1s 980us/sample - loss: 0.6918 - acc: 0.5390 - val_loss: 0.6737 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1s 989us/sample - loss: 0.3226 - acc: 0.9170 - val_loss: 0.1435 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1s 983us/sample - loss: 0.0979 - acc: 1.0000 - val_loss: 0.0658 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0478 - acc: 1.0000 - val_loss: 0.0322 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('Building STATELESS model...')\n",
    "max_len = 10\n",
    "batch_size = 11\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(20, 1), return_sequences=False, stateful=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train.reshape(1000,20,1), X_train[:,0].reshape(1000,1), batch_size=batch_size, epochs=5,\n",
    "          validation_data=(X_test.reshape(200,20,1), X_test[:,0].reshape(200,1)), shuffle=False)\n",
    "score, acc = model.evaluate(X_test.reshape(200,20,1),X_test[:,0].reshape(200,1), batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the splitted sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building STATELESS model...\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 7s 4ms/sample - loss: 0.5096 - acc: 0.7430 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4809 - acc: 0.7500 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4803 - acc: 0.7500 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 7s 4ms/sample - loss: 0.4799 - acc: 0.7500 - val_loss: 0.4790 - val_acc: 0.7500\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4796 - acc: 0.7500 - val_loss: 0.4788 - val_acc: 0.7500\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4794 - acc: 0.7500 - val_loss: 0.4787 - val_acc: 0.7500\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4793 - acc: 0.7500 - val_loss: 0.4786 - val_acc: 0.7500\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4792 - acc: 0.7500 - val_loss: 0.4785 - val_acc: 0.7500\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4791 - acc: 0.7500 - val_loss: 0.4785 - val_acc: 0.7500\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4790 - acc: 0.7500 - val_loss: 0.4784 - val_acc: 0.7500\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4789 - acc: 0.7500 - val_loss: 0.4783 - val_acc: 0.7500\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4789 - acc: 0.7500 - val_loss: 0.4783 - val_acc: 0.7500\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4788 - acc: 0.7500 - val_loss: 0.4782 - val_acc: 0.7500\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4788 - acc: 0.7500 - val_loss: 0.4782 - val_acc: 0.7500\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 7s 3ms/sample - loss: 0.4788 - acc: 0.7500 - val_loss: 0.4782 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "print('Building STATELESS model...')\n",
    "max_len = 10\n",
    "batch_size = 2\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(max_len, 1), return_sequences=False, stateful=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=15,\n",
    "          validation_data=(x_test, y_test), shuffle=False)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences composed of 0s are correctly classified. The subsequences starting with 1 are correctly classified, but the sebsequences of class 1 starting with 0, are wrong classified. Those are the 25% of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long range memory required to classify the sequences correctly has been lost because of the sequences' partition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATEFUL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build STATEFUL model...\n"
     ]
    }
   ],
   "source": [
    "print('Build STATEFUL model...')\n",
    "max_len = 10\n",
    "n_partitions = 2\n",
    "batch = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, batch_input_shape=(batch, max_len, 1), return_sequences=False, stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "accuracy training = 0.9769999980926514\n",
      "loss training = 0.07070142775774002\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 0.0010626701405271888\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 0.00050863076467067\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 0.0002185764751629904\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 0.00012640494969673455\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 6.696998025290668e-05\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 4.0605660615256056e-05\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 2.2503991203848273e-05\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 1.3922774996899534e-05\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 7.854175237298477e-06\n",
      "___________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "for epoch in range(5):\n",
    "    mean_tr_acc = []\n",
    "    mean_tr_loss = []\n",
    "    for i in range(0,x_train.shape[0],n_partitions):\n",
    "        #print(i)\n",
    "        for j in range(n_partitions):\n",
    "            #print(j)\n",
    "            tr_loss, tr_acc = model.train_on_batch(x_train[i+j,:,:].reshape(1,max_len,1), y_train[i+j,:].reshape(1,1))\n",
    "            mean_tr_acc.append(tr_acc)\n",
    "            mean_tr_loss.append(tr_loss)\n",
    "        model.reset_states()    \n",
    "    print('accuracy training = {}'.format(np.mean(mean_tr_acc)))\n",
    "    print('loss training = {}'.format(np.mean(mean_tr_loss)))\n",
    "    print('___________________________________')\n",
    "\n",
    "    mean_te_acc = []\n",
    "    mean_te_loss = []\n",
    "    for i in range(0,x_test.shape[0],n_partitions):\n",
    "        for j in range(n_partitions):\n",
    "            te_loss, te_acc = model.test_on_batch(x_test[i+j,:,:].reshape(1,max_len,1), y_test[i+j,:].reshape(1,1))\n",
    "            mean_te_acc.append(te_acc)\n",
    "            mean_te_loss.append(te_loss)\n",
    "        model.reset_states()\n",
    "\n",
    "    print('accuracy testing = {}'.format(np.mean(mean_te_acc)))\n",
    "    print('loss testing = {}'.format(np.mean(mean_te_loss)))\n",
    "    print('___________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code was a bit more difficult to write because we have to manually call **model.reset_states()** at each new sequence processed. Another method to do that is to write a callback that reset the states at each sequence like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 14s 7ms/sample - loss: 0.0871 - acc: 0.9595\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 11s 6ms/sample - loss: 5.0814e-04 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 11s 6ms/sample - loss: 1.2809e-04 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 11s 6ms/sample - loss: 4.1980e-05 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 11s 6ms/sample - loss: 1.4765e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f48653b5198>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "n_partitions = 2\n",
    "class ResetStatesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if self.counter % n_partitions == 0:\n",
    "            self.model.reset_states()\n",
    "        self.counter += 1\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(LSTM(10, batch_input_shape=(batch, max_len, 1), return_sequences=False, stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, callbacks=[ResetStatesCallback()], batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset for validation have a different batchsize, the best way to solve it, is to create a new model with the new batchsize and transfer to it the weights of the trained model.\n",
    "\n",
    "***Example**: The following code does not have relation with the previous examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network\n",
    "n_batch = 3\n",
    "n_epoch = 1000\n",
    "n_neurons = 10\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit network\n",
    "for i in range(n_epoch):\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# re-define the batch size\n",
    "n_batch = 1\n",
    "# re-define model\n",
    "new_model = Sequential()\n",
    "new_model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "new_model.add(Dense(1))\n",
    "# copy weights\n",
    "old_weights = model.get_weights()\n",
    "new_model.set_weights(old_weights)\n",
    "# compile model\n",
    "new_model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
